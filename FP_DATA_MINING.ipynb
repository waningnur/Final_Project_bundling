{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRyfsyoZmgPy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded data to examine its structure\n",
        "file_path = 'sales_data2.csv'\n",
        "sales_data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "sales_data.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the data with the correct delimiter\n",
        "sales_data = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Display the first few rows of the corrected dataset\n",
        "sales_data.head()\n"
      ],
      "metadata": {
        "id": "ccaeuVLCm6jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split ITEMS column into lists of items for each transaction\n",
        "sales_data['ITEMS'] = sales_data['ITEMS'].apply(lambda x: [item.strip() for item in x.split(',')])\n",
        "\n",
        "# Display the formatted dataset\n",
        "sales_data.head()\n"
      ],
      "metadata": {
        "id": "c2yp3zaQm9it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# Prepare data for Apriori: list of transactions\n",
        "transactions = sales_data['ITEMS'].tolist()\n",
        "\n",
        "# Define parameters for Apriori\n",
        "min_support = 0.1  # Minimum support threshold (e.g., 30%)\n",
        "min_confidence = 0.9  # Minimum confidence threshold (e.g., 70%)\n",
        "n_transactions = len(transactions)\n",
        "\n",
        "# Step 1: Generate frequent itemsets\n",
        "def get_frequent_itemsets(transactions, min_support):\n",
        "    item_support = defaultdict(int)\n",
        "\n",
        "    # Count individual items and itemsets\n",
        "    for transaction in transactions:\n",
        "        for itemset_size in range(1, len(transaction) + 1):\n",
        "            for itemset in combinations(transaction, itemset_size):\n",
        "                item_support[frozenset(itemset)] += 1\n",
        "\n",
        "    # Filter itemsets by support threshold\n",
        "    frequent_itemsets = {itemset: count / n_transactions\n",
        "                         for itemset, count in item_support.items()\n",
        "                         if count / n_transactions >= min_support}\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Step 2: Generate association rules\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets.keys():\n",
        "        if len(itemset) > 1:\n",
        "            for antecedent in combinations(itemset, len(itemset) - 1):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                antecedent_support = sum(1 for t in transactions if antecedent <= set(t)) / len(transactions)\n",
        "                rule_support = frequent_itemsets[itemset]\n",
        "                confidence = rule_support / antecedent_support\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append((antecedent, consequent, confidence))\n",
        "    return rules\n",
        "\n",
        "# Execute Apriori\n",
        "frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
        "rules = generate_rules(frequent_itemsets, transactions, min_confidence)\n",
        "\n",
        "# Output results\n",
        "frequent_itemsets_output = [(set(itemset), support) for itemset, support in frequent_itemsets.items()]\n",
        "association_rules_output = [(set(antecedent), set(consequent), confidence) for antecedent, consequent, confidence in rules]\n",
        "\n",
        "frequent_itemsets_output, association_rules_output\n"
      ],
      "metadata": {
        "id": "aYevz1DunJ42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the newly uploaded file to process the Apriori algorithm on it\n",
        "file_path = 'sales_data2.csv'\n",
        "sales_data = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Ensure data is properly formatted\n",
        "sales_data['ITEMS'] = sales_data['ITEMS'].apply(lambda x: [item.strip() for item in x.split(',')])\n",
        "\n",
        "# Prepare transactions list for Apriori\n",
        "transactions = sales_data['ITEMS'].tolist()\n",
        "\n",
        "# Parameters for Apriori\n",
        "min_support = 0.1  # Minimum support threshold (e.g., 30%)\n",
        "min_confidence = 0.9  # Minimum confidence threshold (e.g., 70%)\n",
        "n_transactions = len(transactions)\n",
        "\n",
        "# Step 1: Generate frequent itemsets\n",
        "def get_frequent_itemsets(transactions, min_support):\n",
        "    item_support = defaultdict(int)\n",
        "\n",
        "    # Count individual items and itemsets\n",
        "    for transaction in transactions:\n",
        "        for itemset_size in range(1, len(transaction) + 1):\n",
        "            for itemset in combinations(transaction, itemset_size):\n",
        "                item_support[frozenset(itemset)] += 1\n",
        "\n",
        "    # Filter itemsets by support threshold\n",
        "    frequent_itemsets = {itemset: count / n_transactions\n",
        "                         for itemset, count in item_support.items()\n",
        "                         if count / n_transactions >= min_support}\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Step 2: Generate association rules\n",
        "def generate_rules(frequent_itemsets, transactions, min_confidence):\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets.keys():\n",
        "        if len(itemset) > 1:\n",
        "            for antecedent in combinations(itemset, len(itemset) - 1):\n",
        "                antecedent = frozenset(antecedent)\n",
        "                consequent = itemset - antecedent\n",
        "                antecedent_support = sum(1 for t in transactions if antecedent <= set(t)) / len(transactions)\n",
        "                rule_support = frequent_itemsets[itemset]\n",
        "                confidence = rule_support / antecedent_support\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append((antecedent, consequent, confidence))\n",
        "    return rules\n",
        "\n",
        "# Execute Apriori\n",
        "frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
        "rules = generate_rules(frequent_itemsets, transactions, min_confidence)\n",
        "\n",
        "# Prepare results\n",
        "frequent_itemsets_output = [(set(itemset), support) for itemset, support in frequent_itemsets.items()]\n",
        "association_rules_output = [(set(antecedent), set(consequent), confidence) for antecedent, consequent, confidence in rules]\n",
        "\n",
        "frequent_itemsets_output, association_rules_output\n"
      ],
      "metadata": {
        "id": "k92ggkOYn_TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "file_path = 'sales_data2.csv'\n",
        "sales_data = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "# Ensure data is properly formatted\n",
        "sales_data['ITEMS'] = sales_data['ITEMS'].apply(lambda x: [item.strip() for item in x.split(',')])\n",
        "\n",
        "# Prepare transactions list for Apriori\n",
        "transactions = sales_data['ITEMS'].tolist()\n",
        "\n",
        "# Parameter\n",
        "min_support = 0.1  # Minimum support (30%)\n",
        "n_transactions = len(transactions)\n",
        "\n",
        "# Fungsi untuk menghitung Frequent Itemsets\n",
        "def get_frequent_itemsets(transactions, min_support):\n",
        "    item_support = defaultdict(int)\n",
        "\n",
        "    # Menghitung jumlah kemunculan itemset\n",
        "    for transaction in transactions:\n",
        "        for size in range(2, len(transaction) + 1):  # Hanya pasangan atau lebih\n",
        "            for itemset in combinations(transaction, size):\n",
        "                item_support[frozenset(itemset)] += 1\n",
        "\n",
        "    # Filter itemset berdasarkan min_support\n",
        "    frequent_itemsets = {\n",
        "        itemset: count / n_transactions\n",
        "        for itemset, count in item_support.items()\n",
        "        if count / n_transactions >= min_support\n",
        "    }\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n",
        "# Eksekusi\n",
        "frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(\"Frequent Itemsets (Item yang sering dibeli bersama):\")\n",
        "for itemset, support in frequent_itemsets.items():\n",
        "    print(f\"{set(itemset)}: {support:.2f}\")\n"
      ],
      "metadata": {
        "id": "Mq10zSTBwbsd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}